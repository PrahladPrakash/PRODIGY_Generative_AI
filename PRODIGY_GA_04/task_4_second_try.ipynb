{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, Input, Concatenate, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras import Model\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import logging\n",
        "from typing import Tuple, List\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "m0QNxVD9trBy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU memory growth to prevent memory allocation issues\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "metadata": {
        "id": "5v7ofv0_lekA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "Jl1qhnicl8-5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageProcessor:\n",
        "    \"\"\"Handles image loading and preprocessing operations.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def load_and_split_image(image_file: str) -> Tuple[tf.Tensor, tf.Tensor]:\n",
        "        \"\"\"\n",
        "        Load and split an image into input and target components.\n",
        "\n",
        "        Args:\n",
        "            image_file: Path to the image file\n",
        "\n",
        "        Returns:\n",
        "            Tuple of input and target images as tensors\n",
        "        \"\"\"\n",
        "        try:\n",
        "            image = tf.io.read_file(image_file)\n",
        "            image = tf.image.decode_jpeg(image)\n",
        "\n",
        "            # Split image into two halves\n",
        "            width = tf.shape(image)[1] // 2\n",
        "            input_image = image[:, :width, :]\n",
        "            target_image = image[:, width:, :]\n",
        "\n",
        "            # Normalize images to [-1, 1]\n",
        "            input_image = (tf.cast(input_image, tf.float32) / 127.5) - 1\n",
        "            target_image = (tf.cast(target_image, tf.float32) / 127.5) - 1\n",
        "\n",
        "            return input_image, target_image\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing image {image_file}: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "YAGixWVtmKoi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(Model):\n",
        "    \"\"\"Generator model for image translation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
        "        self._build_encoder()\n",
        "        self._build_decoder()\n",
        "\n",
        "    def _build_encoder(self):\n",
        "        \"\"\"Build encoder layers.\"\"\"\n",
        "        self.down_stack = [\n",
        "            self._downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n",
        "            self._downsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "            self._downsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "            self._downsample(512, 4),  # (batch_size, 16, 16, 512)\n",
        "        ]\n",
        "\n",
        "    def _build_decoder(self):\n",
        "        \"\"\"Build decoder layers.\"\"\"\n",
        "        self.up_stack = [\n",
        "            self._upsample(256, 4),  # (batch_size, 32, 32, 256)\n",
        "            self._upsample(128, 4),  # (batch_size, 64, 64, 128)\n",
        "            self._upsample(64, 4),   # (batch_size, 128, 128, 64)\n",
        "        ]\n",
        "\n",
        "        self.last = Conv2DTranspose(\n",
        "            3, 4, strides=2, padding='same',\n",
        "            kernel_initializer=self.initializer,\n",
        "            activation='tanh'\n",
        "        )\n",
        "\n",
        "    def _downsample(self, filters: int, size: int, apply_batchnorm: bool = True):\n",
        "        \"\"\"Create a downsampling layer.\"\"\"\n",
        "        initializer = self.initializer\n",
        "\n",
        "        result = tf.keras.Sequential()\n",
        "        result.add(Conv2D(filters, size, strides=2, padding='same',\n",
        "                         kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "        if apply_batchnorm:\n",
        "            result.add(BatchNormalization())\n",
        "\n",
        "        result.add(LeakyReLU())\n",
        "        return result\n",
        "\n",
        "    def _upsample(self, filters: int, size: int, apply_dropout: bool = False):\n",
        "        \"\"\"Create an upsampling layer.\"\"\"\n",
        "        initializer = self.initializer\n",
        "\n",
        "        result = tf.keras.Sequential()\n",
        "        result.add(Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                 kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "        result.add(BatchNormalization())\n",
        "\n",
        "        if apply_dropout:\n",
        "            result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "        result.add(tf.keras.layers.ReLU())\n",
        "        return result\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        \"\"\"Forward pass of the generator.\"\"\"\n",
        "        skips = []\n",
        "\n",
        "        # Encoder\n",
        "        for down in self.down_stack:\n",
        "            x = down(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        skips = reversed(skips[:-1])\n",
        "\n",
        "        # Decoder\n",
        "        for up, skip in zip(self.up_stack, skips):\n",
        "            x = up(x)\n",
        "            x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "        x = self.last(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RFgxEHyBmLBL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same')\n",
        "        self.leaky_relu1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        # Assuming BatchNormalization is applied after conv3\n",
        "        self.bn = BatchNormalization()  # Create BatchNormalization instance here\n",
        "        self.conv2 = tf.keras.layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same')\n",
        "        # Initialize BatchNormalization layers outside the call method\n",
        "        self.batch_norm1 = tf.keras.layers.BatchNormalization()\n",
        "        self.batch_norm2 = tf.keras.layers.BatchNormalization()\n",
        "        self.leaky_relu2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
        "        self.initializer = tf.random_normal_initializer(0., 0.02)\n",
        "        self._build_model()\n",
        "\n",
        "    def _build_model(self):\n",
        "        \"\"\"Build discriminator architecture.\"\"\"\n",
        "        self.conv1 = Conv2D(64, 4, strides=2, padding='same',\n",
        "                           kernel_initializer=self.initializer)\n",
        "        self.conv2 = Conv2D(128, 4, strides=2, padding='same',\n",
        "                           kernel_initializer=self.initializer)\n",
        "        self.conv3 = Conv2D(256, 4, strides=2, padding='same',\n",
        "                           kernel_initializer=self.initializer)\n",
        "        self.output_layer = Conv2D(1, 4, padding='same',\n",
        "                                 kernel_initializer=self.initializer)\n",
        "\n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"Forward pass of the discriminator.\"\"\"\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.leaky_relu1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch_norm2(x, training=training) # Pass training argument\n",
        "        x = self.leaky_relu2(x)\n",
        "        x = self.conv1(inputs)\n",
        "        x = LeakyReLU()(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.bn(x, training=training)  # Use the pre-created instance\n",
        "        x = LeakyReLU()(x)\n",
        "\n",
        "        # Concatenate input_image and target along the channels dimension\n",
        "        x = tf.concat(inputs, axis=-1)  # or axis=3 if channels are last\n",
        "\n",
        "        x = self.conv1(x)  # Now pass the concatenated tensor to conv1\n",
        "        x = LeakyReLU()(x)\n",
        "\n",
        "        # Concatenate the input and target images along the channel dimension\n",
        "        x = tf.concat(inputs, axis=-1) # Assuming inputs is a list [input_image, target]\n",
        "        x = self.conv1(x)  # Pass the concatenated tensor to the first layer\n",
        "\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "S2vrCUU6m-t0"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GANTrainer:\n",
        "    def __init__(self, generator, discriminator):\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "        self.disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "    @tf.function\n",
        "    def train_step(self, input_image, target):\n",
        "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "            gen_output = self.generator(input_image, training=True)\n",
        "\n",
        "            # Concatenate input_image and target along the channel dimension\n",
        "            real_images = tf.concat([input_image, target], axis=-1)\n",
        "            fake_images = tf.concat([input_image, gen_output], axis=-1)\n",
        "\n",
        "            disc_real_output = self.discriminator(real_images, training=True)\n",
        "            disc_fake_output = self.discriminator(fake_images, training=True)\n",
        "\n",
        "            gen_loss = self._generator_loss(disc_fake_output, gen_output, target)\n",
        "            disc_loss = self._discriminator_loss(disc_real_output, disc_fake_output)\n",
        "\n",
        "        gen_gradients = gen_tape.gradient(gen_loss, self.generator.trainable_variables)\n",
        "        disc_gradients = disc_tape.gradient(disc_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "        self.gen_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "        self.disc_optimizer.apply_gradients(zip(disc_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "        return gen_loss, disc_loss\n",
        "\n",
        "    def _generator_loss(self, disc_generated_output: tf.Tensor, gen_output: tf.Tensor, target: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Calculate generator loss.\"\"\"\n",
        "        gan_loss = self._adversarial_loss(disc_generated_output)\n",
        "        l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "        total_gen_loss = gan_loss + (100 * l1_loss)\n",
        "        return total_gen_loss\n",
        "\n",
        "    def _discriminator_loss(self, disc_real_output: tf.Tensor, disc_generated_output: tf.Tensor) -> tf.Tensor:\n",
        "        \"\"\"Calculate discriminator loss.\"\"\"\n",
        "        real_loss = self._adversarial_loss(disc_real_output, real=True)\n",
        "        generated_loss = self._adversarial_loss(disc_generated_output, real=False)\n",
        "        total_disc_loss = real_loss + generated_loss\n",
        "        return total_disc_loss\n",
        "\n",
        "    def _adversarial_loss(self, output: tf.Tensor, real: bool = True) -> tf.Tensor:\n",
        "        \"\"\"Calculate adversarial loss.\"\"\"\n",
        "        if real:\n",
        "            return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.ones_like(output), output))\n",
        "        else:\n",
        "            return tf.reduce_mean(tf.keras.losses.binary_crossentropy(tf.zeros_like(output), output))\n"
      ],
      "metadata": {
        "id": "rkIl-3TgooTV"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(image_path: str, target_path: str, batch_size: int = 32) -> tf.data.Dataset:\n",
        "    \"\"\"\n",
        "    Load and prepare the dataset for training.\n",
        "\n",
        "    Args:\n",
        "        image_path: Directory containing input images\n",
        "        target_path: Directory containing target images\n",
        "        batch_size: Number of samples per batch\n",
        "\n",
        "    Returns:\n",
        "        tf.data.Dataset: Prepared dataset for training\n",
        "    \"\"\"\n",
        "    image_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        image_path,\n",
        "        label_mode=None,\n",
        "        image_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    target_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "        target_path,\n",
        "        label_mode=None,\n",
        "        image_size=(256, 256),\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    train_dataset = tf.data.Dataset.zip((image_dataset, target_dataset))\n",
        "\n",
        "    def preprocess(input_image, target_image):\n",
        "        input_image = tf.cast(input_image, tf.float32) / 127.5 - 1\n",
        "        target_image = tf.cast(target_image, tf.float32) / 127.5 - 1\n",
        "        return input_image, target_image\n",
        "\n",
        "    return train_dataset.map(preprocess).cache().prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "6FkUsSTkpAu9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_sample(generator: Generator, test_input: tf.Tensor, test_target: tf.Tensor):\n",
        "    \"\"\"\n",
        "    Display a sample of input, target, and generated images.\n",
        "\n",
        "    Args:\n",
        "        generator: Trained generator model\n",
        "        test_input: Input image tensor\n",
        "        test_target: Target image tensor\n",
        "    \"\"\"\n",
        "    generated_image = generator(test_input, training=False)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    images = [test_input[0], test_target[0], generated_image[0]]\n",
        "    titles = ['Input', 'Target', 'Generated']\n",
        "\n",
        "    for i, (image, title) in enumerate(zip(images, titles)):\n",
        "        plt.subplot(1, 3, i + 1)\n",
        "        plt.title(title)\n",
        "        plt.imshow(image * 0.5 + 0.5)  # Denormalize the image\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "dd6NvuCEpFII"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# Upload the zip file file A\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the name of the uploaded zip file (assuming only one file was uploaded)\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(io.BytesIO(uploaded[zip_filename]), 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/') # Extract to '/content/' directory\n",
        "\n",
        "print(f\"Folder '{zip_filename[:-4]}' uploaded and extracted to '/content/'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "XwcMdAFipdMC",
        "outputId": "c24506cf-e6b6-4436-be56-f12741446958"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ca12f17e-5f00-435e-93e5-e847e68e25e1\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ca12f17e-5f00-435e-93e5-e847e68e25e1\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trainA.zip to trainA.zip\n",
            "Folder 'trainA' uploaded and extracted to '/content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the zip file file B\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the name of the uploaded zip file (assuming only one file was uploaded)\n",
        "zip_filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(io.BytesIO(uploaded[zip_filename]), 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/') # Extract to '/content/' directory\n",
        "\n",
        "print(f\"Folder '{zip_filename[:-4]}' uploaded and extracted to '/content/'\")"
      ],
      "metadata": {
        "id": "EP-wQBeypmVM",
        "outputId": "6217293b-dbff-4c1e-d475-b421a4f15c4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-633db29e-7fdd-47ee-a217-2db3c9fd8d19\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-633db29e-7fdd-47ee-a217-2db3c9fd8d19\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving trainB.zip to trainB.zip\n",
            "Folder 'trainB' uploaded and extracted to '/content/'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set path\n",
        "# Update image_path and target_path to point to the extracted folders\n",
        "image_path = os.path.join('/content/', 'trainA/') # Use os.path.join for platform independence\n",
        "target_path = os.path.join('/content/', 'trainB/')\n",
        "\n",
        "def main():\n",
        "    # Load dataset\n",
        "    train_dataset = load_dataset(image_path, target_path)\n",
        "\n",
        "    # Create models\n",
        "    generator = Generator()\n",
        "    discriminator = Discriminator()\n",
        "\n",
        "    # Create trainer\n",
        "    trainer = GANTrainer(generator, discriminator)\n",
        "\n",
        "    # Training loop\n",
        "    epochs = 50\n",
        "    for epoch in range(epochs):\n",
        "        for input_image, target in train_dataset:\n",
        "            gen_loss, disc_loss = trainer.train_step(input_image, target)\n",
        "\n",
        "        # Log progress\n",
        "        logger.info(f\"Epoch {epoch + 1}/{epochs}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
        "\n",
        "        # Display sample every 10 epochs\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            display_sample(generator, input_image, target)\n",
        "\n",
        "    # Save the trained generator\n",
        "    generator.save('trained_generator.h5')\n",
        "    logger.info(\"Training completed. Generator saved as 'trained_generator.h5'\")"
      ],
      "metadata": {
        "id": "L6BHvrEOpIaX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2wExnBs_pMIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74d8be87-72e2-4ef9-bf42-8bb2a2a3baab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 files.\n",
            "Found 10000 files.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/layer.py:372: UserWarning: `build()` was called on layer 'discriminator_12', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aHoUwy1W8O0B"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
